# Status

- [ ] gets vault base url for all systems
- [x] gets links to all vaults A-Z for given system
- [x] gets game data page links from vault link
- [x] gets download links with md5 hash for all games in given vaults
- [ ] downloads games
- [ ] scrape package
- [ ] download package
- [ ] handler package
- [ ] respecful scraping
- [ ] proxied scraping
- [ ] proxied download
- [ ] multithreaded downloads

# Thoughts and Progress Notes

### Initial building and idea (multi day)
I recently got a steam deck, and have been setting up my NAS to work hand in hand with the device.
I need a large library of ROMs, and have the drive space set up, but I don't want to manually downlad.
Started looking into scraping, found golang to be the best option.
Started following [this guide][scrape-guide] and developed basic working script for the gamecube section of vimm.net.
Tested and working with Nintendo 64 page as well, could be viable to scrape entire site


### 10/17/2022
* scraping for links is easy, downloading is harder than intended
* download seems to work on some cookie system
    * cookie only contains "count" and "awsuser" at page load
    * once download button pressed, remainder of cookie is requested (if not previously attached and not expired)
        * calls `pica.js`, then a url that seems to be generated by pica.js (different every time)
        * generated url adds remainder of cookie
    * cookie is passed at time of download for verification
    * returns error if link is directly followed without cookie generation

### 10/18/2022
I started making requests through insomnia andfound that you can make a request with out a cookie
with only 2 required headers:
* `Referer: https://vimm.net/`
* `User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:107.0) Gecko/20100101 Firefox/107.0`
User agent can be more generic, but I am using a Mac to develop this script, so i just used that.
Now I'm waiting for the internet data plan to reset for mass download testing.

* reasearched making go project more modular, decided creating go module is easiest
    * [reference][go-packages]
    * package to scrape each rom site
        * respectful (wait time between requests)
        * simple proxy to keep admin from finding scraper
        * allows for portablility, can add more sites
    * package to download from each site
        * round robin proxy
        * multi-threaded, each thread on different proxy
    * handler package for logging and custom errors
    * main ui that allows user to choose some things
        * scrape individual console or all console?
        * download all games, or just games in list generated by scraper that is filtered by user
        * site preference? 

[go-packages]: https://golangbot.com/go-packages/
[scrape-guide]: https://oxylabs.io/blog/golang-web-scraper